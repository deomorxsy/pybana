name: CI Pipeline for Spark and Airflow

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  airflow-spark-job:
    runs-on: [self-hosted, airflow-runner]
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      # setup full k8s context
      - uses: debianmaster/actions-k3s@master
        id: setup-k8s
        run: |
          if [ -n "${{ runner.name }}" ]; then
            echo "Using Self-Hosted Action Runner Controller (ARC) on local kubernetes cluster"
            echo "K8S_CONTEXT=local" >> $GITHUB_ENV
          else
            echo "using k3s on Github-Hosted Action Runner"
            echo "K8S_CONTEXT=k3s" >> $GITHUB_ENV
          fi

      - name: Setup k3s (Github-Hosted Action Runner)
        if:  env.K8S_CONTEXT == 'k3s'
      # setup k3s cluster
        uses: debianmaster/actions-k3s@master
        id: k3s
        with:
          version: 'latest'

      - name: Deploy Airflow DAG
        run: |
          kubectl cp ./dags/main.py airflow-pod:/opt/airflow/dags/
          kubectl exec airflow-pod -- airflow dags trigger my_pyspark_dag

      - name: Submit Spark Job
        run: |
          kubectl apply -f spark-job.yaml

      - name: Monitor Job Status
        run: |
          kubectl logs -f $(kubectl get pod -l spark-role=driver -o jsonpath='{.items[0].metadata.name}')

      - name: Deploy DAG to Airflow
        run: |
          cp -rf ../pybana /home/debian/services/airflow-docker/dags

